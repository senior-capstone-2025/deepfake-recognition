{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb86182b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f268eeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add StyleFlow path\n",
    "styleflow_path = os.path.abspath('external/StyleFlow')\n",
    "if styleflow_path not in sys.path:\n",
    "    sys.path.insert(0, styleflow_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032e3570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import StyleGRU from submodule with explicit module path\n",
    "from external.StyleFlow.StyleGRU.model import StyleGRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6c13ad",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cd385d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleAttention(nn.Module):\n",
    "    \"\"\"Style Attention module to fuse StyleGRU features with ResNet features\"\"\"\n",
    "    def __init__(self, style_dim, content_dim, output_dim):\n",
    "        super(StyleAttention, self).__init__()\n",
    "        self.style_projection = nn.Linear(style_dim, output_dim)\n",
    "        self.content_projection = nn.Linear(content_dim, output_dim)\n",
    "        self.attention = nn.MultiheadAttention(output_dim, num_heads=8)\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(output_dim * 2, output_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(output_dim, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, style_features, content_features):\n",
    "        # Project both feature types to the same dimension\n",
    "        style_proj = self.style_projection(style_features)\n",
    "        content_proj = self.content_projection(content_features)\n",
    "        \n",
    "        # Use attention to fuse the features\n",
    "        attn_output, _ = self.attention(\n",
    "            query=content_proj.unsqueeze(0),\n",
    "            key=style_proj.unsqueeze(0),\n",
    "            value=style_proj.unsqueeze(0)\n",
    "        )\n",
    "        \n",
    "        # Concatenate attention output with content features\n",
    "        fused_features = torch.cat([content_proj, attn_output.squeeze(0)], dim=1)\n",
    "        \n",
    "        # Final fusion\n",
    "        output = self.fusion(fused_features)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb2c1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional encoding for transformer\"\"\"\n",
    "    def __init__(self, d_model, max_len=16):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # Create positional encoding\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Add positional encoding to input\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66876293",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalTransformerEncoder(nn.Module):\n",
    "    \"\"\"Temporal transformer encoder for sequence modeling\"\"\"\n",
    "    def __init__(self, input_dim, d_model=512, nhead=8, num_layers=4, dim_feedforward=2048, dropout=0.1):\n",
    "        super(TemporalTransformerEncoder, self).__init__()\n",
    "        \n",
    "        self.input_projection = nn.Linear(input_dim, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Project input to transformer dimension\n",
    "        x = self.input_projection(x)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.positional_encoding(x)\n",
    "        \n",
    "        # Apply transformer encoder\n",
    "        output = self.transformer_encoder(x, src_key_padding_mask=mask)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ab1807",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalDeepfakeDetector(nn.Module):\n",
    "    \"\"\"Complete multimodal deepfake detection model with style attention and transformer\"\"\"\n",
    "    def __init__(self, \n",
    "                 style_dim=8192, \n",
    "                 content_dim=512, \n",
    "                 fusion_dim=1024,\n",
    "                 audio_dim=120,\n",
    "                 transformer_dim=512,\n",
    "                 num_heads=8,\n",
    "                 num_layers=4,\n",
    "                 sequence_length=16):\n",
    "        super(MultimodalDeepfakeDetector, self).__init__()\n",
    "        \n",
    "        # Video processing components\n",
    "        self.style_gru = StyleGRU(feature_size=9216)\n",
    "        self.style_attention = StyleAttention(\n",
    "            style_dim=style_dim,\n",
    "            content_dim=content_dim,\n",
    "            output_dim=fusion_dim\n",
    "        )\n",
    "        \n",
    "        # Temporal transformer for sequence modeling\n",
    "        self.temporal_transformer = TemporalTransformerEncoder(\n",
    "            input_dim=fusion_dim,\n",
    "            d_model=transformer_dim,\n",
    "            nhead=num_heads,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        # Video classifier\n",
    "        self.video_classifier = nn.Sequential(\n",
    "            nn.Linear(transformer_dim, transformer_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(transformer_dim // 2, 1)\n",
    "        )\n",
    "        \n",
    "       \n",
    "        \n",
    "        # Final fusion layer for multimodal decision\n",
    "        self.fusion_classifier = nn.Sequential(\n",
    "            nn.Linear(2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Sequence length for temporal processing\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "    def forward(self, style_features, content_features, audio_features=None):\n",
    "        # Process through StyleGRU\n",
    "        _, style_hidden = self.style_gru(style_features)\n",
    "        \n",
    "        # Fuse features with style attention\n",
    "        fused_features = self.style_attention(style_hidden, content_features)\n",
    "        \n",
    "        # Reshape for transformer if needed\n",
    "        if len(fused_features.shape) == 1:\n",
    "            # If we have a single feature vector, expand to sequence\n",
    "            fused_features = fused_features.unsqueeze(0).unsqueeze(0)\n",
    "            fused_features = fused_features.expand(-1, self.sequence_length, -1)\n",
    "        elif len(fused_features.shape) == 2 and fused_features.shape[0] == 1:\n",
    "            # If we have a batch of 1, expand to sequence\n",
    "            fused_features = fused_features.unsqueeze(1)\n",
    "            fused_features = fused_features.expand(-1, self.sequence_length, -1)\n",
    "        \n",
    "        # Process through temporal transformer\n",
    "        transformer_output = self.temporal_transformer(fused_features)\n",
    "        \n",
    "        # Global average pooling over sequence dimension\n",
    "        pooled_output = torch.mean(transformer_output, dim=1)\n",
    "        \n",
    "        # Video classification\n",
    "        video_pred = torch.sigmoid(self.video_classifier(pooled_output))\n",
    "        \n",
    "        # Audio classification if audio features are provided\n",
    "        if audio_features is not None:\n",
    "            audio_pred = self.audio_classifier(audio_features)\n",
    "            \n",
    "            # Combine video and audio predictions\n",
    "            combined_preds = torch.cat([video_pred, audio_pred], dim=1)\n",
    "            final_pred = self.fusion_classifier(combined_preds)\n",
    "            \n",
    "            return final_pred, video_pred, audio_pred\n",
    "        else:\n",
    "            # Return only video prediction if no audio\n",
    "            return video_pred, video_pred, None\n",
    "    \n",
    "    def predict_video(self, video_path, processor):\n",
    "        \"\"\"Make prediction on a video file\"\"\"\n",
    "        # Process video\n",
    "        features = processor.process_video(video_path)\n",
    "        \n",
    "        if features is None:\n",
    "            return None\n",
    "        \n",
    "        # Prepare inputs\n",
    "        style_features = torch.FloatTensor(features['style_features']).unsqueeze(0)\n",
    "        content_features = torch.FloatTensor(features['resnet_features'][0]).unsqueeze(0)\n",
    "        \n",
    "        if len(features['audio_features']) > 0:\n",
    "            # Average audio features across time\n",
    "            audio_features = torch.FloatTensor(np.mean(features['audio_features'], axis=0)).unsqueeze(0)\n",
    "        else:\n",
    "            audio_features = None\n",
    "        \n",
    "        # Move to GPU if available\n",
    "        if torch.cuda.is_available():\n",
    "            style_features = style_features.cuda()\n",
    "            content_features = content_features.cuda()\n",
    "            if audio_features is not None:\n",
    "                audio_features = audio_features.cuda()\n",
    "        \n",
    "        # Make prediction\n",
    "        with torch.no_grad():\n",
    "            final_pred, video_pred, audio_pred = self.forward(\n",
    "                style_features, content_features, audio_features\n",
    "            )\n",
    "        \n",
    "        # Return predictions\n",
    "        result = {\n",
    "            'final_prediction': final_pred.cpu().numpy()[0][0],\n",
    "            'video_prediction': video_pred.cpu().numpy()[0][0],\n",
    "            'audio_prediction': audio_pred.cpu().numpy()[0][0] if audio_pred is not None else None,\n",
    "            'is_fake': final_pred.cpu().numpy()[0][0] > 0.5\n",
    "        }\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea24940",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056bcd11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
