{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cae49f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import mediapipe as mp\n",
    "import librosa\n",
    "import noisereduce as nr\n",
    "import subprocess\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import torchvision.models as models\n",
    "from torch import nn\n",
    "import webrtcvad\n",
    "import wave\n",
    "import struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d35e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import StyleGRU from the codebase\n",
    "from external.StyleFlow.StyleGRU.model import StyleGRU\n",
    "from external.StyleFlow.StyleGRU.dataloader.triplet_clip_loader import get_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9724689",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoAudioProcessor:\n",
    "    def __init__(self, \n",
    "                 output_dir='processed_data',\n",
    "                 frame_rate=1,\n",
    "                 face_confidence=0.5,\n",
    "                 audio_sample_rate=16000,\n",
    "                 style_gru_model_path=None,\n",
    "                 style_feature_size=9216,\n",
    "                 sequence_length=32):\n",
    "        \n",
    "        self.output_dir = output_dir\n",
    "        self.frame_rate = frame_rate\n",
    "        self.face_confidence = face_confidence\n",
    "        self.audio_sample_rate = audio_sample_rate\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        # Create output directories\n",
    "        os.makedirs(os.path.join(output_dir, 'video_features'), exist_ok=True)\n",
    "        os.makedirs(os.path.join(output_dir, 'audio_features'), exist_ok=True)\n",
    "        os.makedirs(os.path.join(output_dir, 'combined_features'), exist_ok=True)\n",
    "        \n",
    "        # Initialize MediaPipe face mesh\n",
    "        self.mp_face_mesh = mp.solutions.face_mesh\n",
    "        self.face_mesh = self.mp_face_mesh.FaceMesh(\n",
    "            static_image_mode=False,\n",
    "            max_num_faces=1,\n",
    "            min_detection_confidence=face_confidence,\n",
    "            min_tracking_confidence=face_confidence\n",
    "        )\n",
    "        \n",
    "        # Initialize ResNet model for feature extraction\n",
    "        self.resnet = models.video.r3d_18(pretrained=True)\n",
    "        self.resnet.fc = nn.Identity()  # Remove classification layer\n",
    "        self.resnet.eval()\n",
    "        if torch.cuda.is_available():\n",
    "            self.resnet = self.resnet.cuda()\n",
    "        \n",
    "        # Initialize StyleGRU model if path is provided\n",
    "        self.style_gru = None\n",
    "        if style_gru_model_path and os.path.exists(style_gru_model_path):\n",
    "            self.style_gru = StyleGRU(feature_size=style_feature_size)\n",
    "            self.style_gru.load_state_dict(torch.load(style_gru_model_path))\n",
    "            self.style_gru.eval()\n",
    "            if torch.cuda.is_available():\n",
    "                self.style_gru = self.style_gru.cuda()\n",
    "        \n",
    "        # Initialize VAD for voice activity detection\n",
    "        self.vad = webrtcvad.Vad(3)  # Aggressiveness level 3 (highest)\n",
    "    \n",
    "    def extract_frames(self, video_path):\n",
    "        \"\"\"Extract frames from video at specified frame rate\"\"\"\n",
    "        frames = []\n",
    "        timestamps = []\n",
    "        \n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        duration = total_frames / fps\n",
    "        \n",
    "        # Calculate frame interval based on desired frame rate\n",
    "        frame_interval = int(fps / self.frame_rate)\n",
    "        \n",
    "        current_frame = 0\n",
    "        while cap.isOpened():\n",
    "            if current_frame >= total_frames:\n",
    "                break\n",
    "                \n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, current_frame)\n",
    "            ret, frame = cap.read()\n",
    "            \n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            # Convert to RGB (MediaPipe uses RGB)\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(frame_rgb)\n",
    "            timestamps.append(current_frame / fps)\n",
    "            \n",
    "            current_frame += frame_interval\n",
    "        \n",
    "        cap.release()\n",
    "        return frames, timestamps, duration\n",
    "    \n",
    "    def detect_and_crop_face(self, frame):\n",
    "        \"\"\"Detect facial landmarks and crop face region\"\"\"\n",
    "        h, w = frame.shape[:2]\n",
    "        results = self.face_mesh.process(frame)\n",
    "        \n",
    "        if not results.multi_face_landmarks:\n",
    "            return None, None\n",
    "        \n",
    "        # Get the first face\n",
    "        face_landmarks = results.multi_face_landmarks[0]\n",
    "        \n",
    "        # Extract landmark coordinates\n",
    "        landmarks = []\n",
    "        for landmark in face_landmarks.landmark:\n",
    "            x, y = int(landmark.x * w), int(landmark.y * h)\n",
    "            landmarks.append((x, y))\n",
    "        \n",
    "        # Calculate bounding box with margin\n",
    "        landmarks = np.array(landmarks)\n",
    "        x_min, y_min = np.min(landmarks, axis=0)\n",
    "        x_max, y_max = np.max(landmarks, axis=0)\n",
    "        \n",
    "        # Add margin (20% of face size)\n",
    "        margin_x = int((x_max - x_min) * 0.2)\n",
    "        margin_y = int((y_max - y_min) * 0.2)\n",
    "        \n",
    "        x_min = max(0, x_min - margin_x)\n",
    "        y_min = max(0, y_min - margin_y)\n",
    "        x_max = min(w, x_max + margin_x)\n",
    "        y_max = min(h, y_max + margin_y)\n",
    "        \n",
    "        # Crop face\n",
    "        face_crop = frame[y_min:y_max, x_min:x_max]\n",
    "        \n",
    "        # Normalize landmarks to the cropped face coordinates\n",
    "        norm_landmarks = landmarks.copy()\n",
    "        norm_landmarks[:, 0] = norm_landmarks[:, 0] - x_min\n",
    "        norm_landmarks[:, 1] = norm_landmarks[:, 1] - y_min\n",
    "        \n",
    "        return face_crop, norm_landmarks\n",
    "    \n",
    "    def extract_resnet_features(self, face_frames):\n",
    "        \"\"\"Extract features using 3D ResNet\"\"\"\n",
    "        if len(face_frames) < 16:  # ResNet3D typically needs at least 16 frames\n",
    "            # Pad with duplicates of the last frame if needed\n",
    "            face_frames.extend([face_frames[-1]] * (16 - len(face_frames)))\n",
    "        \n",
    "        # Prepare frames for ResNet (normalize, resize, etc.)\n",
    "        processed_frames = []\n",
    "        for frame in face_frames:\n",
    "            # Resize to ResNet input size\n",
    "            frame = cv2.resize(frame, (112, 112))\n",
    "            # Convert to float and normalize\n",
    "            frame = frame.astype(np.float32) / 255.0\n",
    "            # Normalize using ImageNet mean and std\n",
    "            frame = (frame - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n",
    "            # Change to channel-first format\n",
    "            frame = np.transpose(frame, (2, 0, 1))\n",
    "            processed_frames.append(frame)\n",
    "        \n",
    "        # Convert to tensor\n",
    "        clip = torch.FloatTensor(np.array(processed_frames))\n",
    "        # Add batch dimension\n",
    "        clip = clip.unsqueeze(0)\n",
    "        \n",
    "        # Move to GPU if available\n",
    "        if torch.cuda.is_available():\n",
    "            clip = clip.cuda()\n",
    "        \n",
    "        # Extract features\n",
    "        with torch.no_grad():\n",
    "            features = self.resnet(clip)\n",
    "        \n",
    "        return features.cpu().numpy().flatten()\n",
    "    \n",
    "    def extract_style_features(self, features_sequence):\n",
    "        \"\"\"Extract temporal style features using StyleGRU\"\"\"\n",
    "        if self.style_gru is None:\n",
    "            return None\n",
    "        \n",
    "        # Convert to tensor\n",
    "        features_tensor = torch.FloatTensor(np.array(features_sequence))\n",
    "        # Add batch dimension\n",
    "        features_tensor = features_tensor.unsqueeze(0)\n",
    "        \n",
    "        # Calculate temporal differences\n",
    "        features_diff = get_diff(features_tensor)\n",
    "        \n",
    "        # Move to GPU if available\n",
    "        if torch.cuda.is_available():\n",
    "            features_diff = features_diff.cuda()\n",
    "        \n",
    "        # Extract features\n",
    "        with torch.no_grad():\n",
    "            _, style_features = self.style_gru(features_diff)\n",
    "        \n",
    "        return style_features.cpu().numpy().flatten()\n",
    "    \n",
    "    def extract_audio(self, video_path, output_audio_path=None):\n",
    "        \"\"\"Extract audio from video using ffmpeg\"\"\"\n",
    "        if output_audio_path is None:\n",
    "            output_audio_path = os.path.splitext(video_path)[0] + '.wav'\n",
    "        \n",
    "        command = [\n",
    "            'ffmpeg', '-i', video_path, '-vn', '-acodec', 'pcm_s16le',\n",
    "            '-ar', str(self.audio_sample_rate), '-ac', '1', output_audio_path,\n",
    "            '-y'  # Overwrite if exists\n",
    "        ]\n",
    "        \n",
    "        subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        return output_audio_path\n",
    "    \n",
    "    def reduce_noise(self, audio_data, sr):\n",
    "        \"\"\"Apply noise reduction to audio\"\"\"\n",
    "        # Reduce noise\n",
    "        reduced_noise = nr.reduce_noise(\n",
    "            y=audio_data,\n",
    "            sr=sr,\n",
    "            prop_decrease=0.75,\n",
    "            stationary=True\n",
    "        )\n",
    "        return reduced_noise\n",
    "    \n",
    "    def detect_voice_activity(self, audio_path):\n",
    "        \"\"\"Detect voice activity in audio using WebRTC VAD\"\"\"\n",
    "        # Open the wave file\n",
    "        wf = wave.open(audio_path, 'rb')\n",
    "        \n",
    "        # Check if the audio format is compatible with VAD\n",
    "        if wf.getnchannels() != 1 or wf.getsampwidth() != 2 or wf.getframerate() not in [8000, 16000, 32000, 48000]:\n",
    "            raise ValueError(\"Audio file must be mono, 16-bit, and 8000/16000/32000/48000 Hz\")\n",
    "        \n",
    "        # Process audio in 30ms chunks\n",
    "        frame_duration_ms = 30\n",
    "        frame_size = int(wf.getframerate() * frame_duration_ms / 1000)\n",
    "        \n",
    "        # Store chunks with voice activity\n",
    "        voice_chunks = []\n",
    "        \n",
    "        while True:\n",
    "            chunk = wf.readframes(frame_size)\n",
    "            if len(chunk) < 2 * frame_size:  # End of file or incomplete chunk\n",
    "                break\n",
    "                \n",
    "            # Check if chunk contains voice\n",
    "            is_speech = self.vad.is_speech(chunk, wf.getframerate())\n",
    "            if is_speech:\n",
    "                voice_chunks.append(chunk)\n",
    "        \n",
    "        wf.close()\n",
    "        \n",
    "        # Combine voice chunks into a single audio stream\n",
    "        if voice_chunks:\n",
    "            voice_audio = b''.join(voice_chunks)\n",
    "            # Convert bytes to audio samples\n",
    "            voice_samples = np.array(struct.unpack(f'{len(voice_audio)//2}h', voice_audio), dtype=np.float32) / 32768.0\n",
    "            return voice_samples\n",
    "        else:\n",
    "            return np.array([])\n",
    "    \n",
    "    def extract_audio_features(self, audio_data, sr):\n",
    "        \"\"\"Extract MFCC features from audio\"\"\"\n",
    "        # Extract MFCCs\n",
    "        mfccs = librosa.feature.mfcc(\n",
    "            y=audio_data,\n",
    "            sr=sr,\n",
    "            n_mfcc=40,\n",
    "            hop_length=512,\n",
    "            n_fft=2048\n",
    "        )\n",
    "        \n",
    "        # Add delta and delta-delta features\n",
    "        delta_mfccs = librosa.feature.delta(mfccs)\n",
    "        delta2_mfccs = librosa.feature.delta(mfccs, order=2)\n",
    "        \n",
    "        # Combine features\n",
    "        audio_features = np.concatenate([mfccs, delta_mfccs, delta2_mfccs], axis=0)\n",
    "        \n",
    "        # Transpose to get time as first dimension\n",
    "        audio_features = audio_features.T\n",
    "        \n",
    "        return audio_features\n",
    "    \n",
    "    def process_video(self, video_path, label=None):\n",
    "        \"\"\"Process video file and extract features\"\"\"\n",
    "        video_name = os.path.basename(video_path).split('.')[0]\n",
    "        print(f\"Processing video: {video_name}\")\n",
    "        \n",
    "        # Extract frames\n",
    "        frames, timestamps, duration = self.extract_frames(video_path)\n",
    "        print(f\"Extracted {len(frames)} frames\")\n",
    "        \n",
    "        # Process frames\n",
    "        face_frames = []\n",
    "        landmarks_list = []\n",
    "        valid_frames = []\n",
    "        \n",
    "        for i, frame in enumerate(frames):\n",
    "            face_crop, landmarks = self.detect_and_crop_face(frame)\n",
    "            if face_crop is not None and landmarks is not None:\n",
    "                face_frames.append(face_crop)\n",
    "                landmarks_list.append(landmarks)\n",
    "                valid_frames.append(i)\n",
    "        \n",
    "        print(f\"Detected faces in {len(face_frames)} frames\")\n",
    "        \n",
    "        if len(face_frames) == 0:\n",
    "            print(f\"No faces detected in {video_name}\")\n",
    "            return None\n",
    "        \n",
    "        # Extract ResNet features for each face frame\n",
    "        resnet_features = []\n",
    "        \n",
    "        # Process in sliding windows of 16 frames with overlap\n",
    "        window_size = 16\n",
    "        stride = 8\n",
    "        \n",
    "        for i in range(0, len(face_frames) - window_size + 1, stride):\n",
    "            window_frames = face_frames[i:i+window_size]\n",
    "            features = self.extract_resnet_features(window_frames)\n",
    "            resnet_features.append(features)\n",
    "        \n",
    "        print(f\"Extracted ResNet features for {len(resnet_features)} windows\")\n",
    "        \n",
    "        # Extract style features if we have enough frames\n",
    "        style_features = None\n",
    "        if len(resnet_features) >= 2 and self.style_gru is not None:\n",
    "            # Ensure we have the right sequence length for StyleGRU\n",
    "            if len(resnet_features) > self.sequence_length:\n",
    "                resnet_features = resnet_features[:self.sequence_length]\n",
    "            elif len(resnet_features) < self.sequence_length:\n",
    "                # Pad with zeros\n",
    "                padding = [np.zeros_like(resnet_features[0]) for _ in range(self.sequence_length - len(resnet_features))]\n",
    "                resnet_features.extend(padding)\n",
    "            \n",
    "            style_features = self.extract_style_features(resnet_features)\n",
    "            print(\"Extracted style features\")\n",
    "        \n",
    "        # Extract audio and process\n",
    "        audio_path = self.extract_audio(video_path)\n",
    "        audio_data, sr = librosa.load(audio_path, sr=self.audio_sample_rate)\n",
    "        \n",
    "        # Reduce noise\n",
    "        audio_data = self.reduce_noise(audio_data, sr)\n",
    "        \n",
    "        # Detect voice activity\n",
    "        voice_data = self.detect_voice_activity(audio_path)\n",
    "        if len(voice_data) > 0:\n",
    "            # Extract audio features\n",
    "            audio_features = self.extract_audio_features(voice_data, sr)\n",
    "            print(f\"Extracted audio features: {audio_features.shape}\")\n",
    "        else:\n",
    "            audio_features = np.array([])\n",
    "            print(\"No voice activity detected\")\n",
    "        \n",
    "        # Clean up temporary audio file\n",
    "        if os.path.exists(audio_path) and audio_path != video_path:\n",
    "            os.remove(audio_path)\n",
    "        \n",
    "        # Save features\n",
    "        output = {\n",
    "            'video_name': video_name,\n",
    "            'duration': duration,\n",
    "            'timestamps': [timestamps[i] for i in valid_frames],\n",
    "            'landmarks': landmarks_list,\n",
    "            'resnet_features': resnet_features,\n",
    "            'style_features': style_features,\n",
    "            'audio_features': audio_features,\n",
    "            'label': label\n",
    "        }\n",
    "        \n",
    "        # Save features\n",
    "        output_path = os.path.join(self.output_dir, 'combined_features', f\"{video_name}.pkl\")\n",
    "        with open(output_path, 'wb') as f:\n",
    "            pickle.dump(output, f)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def process_dataset(self, video_dir, label_map=None):\n",
    "        \"\"\"Process all videos in a directory\"\"\"\n",
    "        video_files = [f for f in os.listdir(video_dir) if f.endswith(('.mp4', '.avi', '.mov'))]\n",
    "        results = []\n",
    "        \n",
    "        for video_file in tqdm(video_files, desc=\"Processing videos\"):\n",
    "            video_path = os.path.join(video_dir, video_file)\n",
    "            \n",
    "            # Determine label if label_map is provided\n",
    "            label = None\n",
    "            if label_map is not None:\n",
    "                for key, value in label_map.items():\n",
    "                    if key in video_file:\n",
    "                        label = value\n",
    "                        break\n",
    "            \n",
    "            result = self.process_video(video_path, label)\n",
    "            if result is not None:\n",
    "                results.append(result)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def prepare_for_stylegru(self, features_list, output_path):\n",
    "        \"\"\"Prepare features for StyleGRU training\"\"\"\n",
    "        real_samples = []\n",
    "        fake_samples = []\n",
    "        \n",
    "        for features in features_list:\n",
    "            if features['label'] == 0:  # Real\n",
    "                real_samples.append(features)\n",
    "            elif features['label'] == 1:  # Fake\n",
    "                fake_samples.append(features)\n",
    "        \n",
    "        # Save prepared data\n",
    "        prepared_data = {\n",
    "            'real_samples': real_samples,\n",
    "            'fake_samples': fake_samples\n",
    "        }\n",
    "        \n",
    "        with open(output_path, 'wb') as f:\n",
    "            pickle.dump(prepared_data, f)\n",
    "        \n",
    "        print(f\"Prepared {len(real_samples)} real samples and {len(fake_samples)} fake samples\")\n",
    "        return prepared_data\n",
    "    \n",
    "    def create_triplets_for_training(self, prepared_data, output_path, num_triplets=10000):\n",
    "        \"\"\"Create triplets for contrastive learning\"\"\"\n",
    "        real_samples = prepared_data['real_samples']\n",
    "        fake_samples = prepared_data['fake_samples']\n",
    "        \n",
    "        triplets = []\n",
    "        \n",
    "        # Ensure we have enough samples\n",
    "        if len(real_samples) < 2 or len(fake_samples) < 1:\n",
    "            print(\"Not enough samples to create triplets\")\n",
    "            return []\n",
    "        \n",
    "        for _ in tqdm(range(num_triplets), desc=\"Creating triplets\"):\n",
    "            # Randomly select two real samples and one fake sample\n",
    "            pos_anchor_idx = np.random.randint(0, len(real_samples))\n",
    "            pos_idx = np.random.randint(0, len(real_samples))\n",
    "            neg_idx = np.random.randint(0, len(fake_samples))\n",
    "            \n",
    "            # Ensure positive anchor and positive are different samples\n",
    "            while pos_anchor_idx == pos_idx and len(real_samples) > 1:\n",
    "                pos_idx = np.random.randint(0, len(real_samples))\n",
    "            \n",
    "            pos_anchor = real_samples[pos_anchor_idx]\n",
    "            pos = real_samples[pos_idx]\n",
    "            neg = fake_samples[neg_idx]\n",
    "            \n",
    "            # Create triplet\n",
    "            triplet = {\n",
    "                'pos_anchor': pos_anchor['resnet_features'],\n",
    "                'pos': pos['resnet_features'],\n",
    "                'neg': neg['resnet_features'],\n",
    "                'labels': [0, 0, 1]  # [pos_anchor_label, pos_label, neg_label]\n",
    "            }\n",
    "            \n",
    "            triplets.append(triplet)\n",
    "        \n",
    "        # Save triplets\n",
    "        with open(output_path, 'wb') as f:\n",
    "            pickle.dump(triplets, f)\n",
    "        \n",
    "        print(f\"Created {len(triplets)} triplets for training\")\n",
    "        return triplets"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
