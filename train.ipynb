{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465ccc87",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44174ade",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(predictions, targets):\n",
    "    \"\"\"Calculate accuracy for binary classification\"\"\"\n",
    "    pred_labels = (predictions > 0.5).float()\n",
    "    correct = (pred_labels == targets).float().sum()\n",
    "    return correct / targets.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6604c232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=10, learning_rate=0.001):\n",
    "    \"\"\"Train the deepfake detection model\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        \n",
    "        for style_features, content_features, audio_features, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
    "            # Move data to device\n",
    "            style_features = style_features.to(device)\n",
    "            content_features = content_features.to(device)\n",
    "            audio_features = audio_features.to(device) if audio_features is not None else None\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            final_pred, video_pred, audio_pred = model(style_features, content_features, audio_features)\n",
    "            \n",
    "            # Calculate losses\n",
    "            final_loss = criterion(final_pred, labels)\n",
    "            video_loss = criterion(video_pred, labels)\n",
    "            \n",
    "            if audio_pred is not None:\n",
    "                audio_loss = criterion(audio_pred, labels)\n",
    "                total_loss = final_loss + 0.5 * (video_loss + audio_loss)\n",
    "            else:\n",
    "                total_loss = final_loss + 0.5 * video_loss\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update statistics\n",
    "            train_loss += total_loss.item()\n",
    "            train_acc += calculate_accuracy(final_pred, labels)\n",
    "        \n",
    "        # Calculate average training metrics\n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc /= len(train_loader)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_acc = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for style_features, content_features, audio_features, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
    "                # Move data to device\n",
    "                style_features = style_features.to(device)\n",
    "                content_features = content_features.to(device)\n",
    "                audio_features = audio_features.to(device) if audio_features is not None else None\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                final_pred, video_pred, audio_pred = model(style_features, content_features, audio_features)\n",
    "                \n",
    "                # Calculate losses\n",
    "                final_loss = criterion(final_pred, labels)\n",
    "                video_loss = criterion(video_pred, labels)\n",
    "                \n",
    "                if audio_pred is not None:\n",
    "                    audio_loss = criterion(audio_pred, labels)\n",
    "                    total_loss = final_loss + 0.5 * (video_loss + audio_loss)\n",
    "                else:\n",
    "                    total_loss = final_loss + 0.5 * video_loss\n",
    "                \n",
    "                # Update statistics\n",
    "                val_loss += total_loss.item()\n",
    "                val_acc += calculate_accuracy(final_pred, labels)\n",
    "        \n",
    "        # Calculate average validation metrics\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc /= len(val_loader)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Print epoch statistics\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f\"Saved best model with validation loss: {val_loss:.4f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74397e94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
